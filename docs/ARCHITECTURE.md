# scancode-rust Architecture

## Overview

scancode-rust is a complete rewrite of [ScanCode Toolkit](https://github.com/aboutcode-org/scancode-toolkit) in Rust, designed as a **drop-in replacement** with all features of the original, but with:

- **Zero bugs**: Leveraging Rust's type system and ownership model
- **Better performance**: Native code, parallel processing, zero-copy parsing
- **Enhanced security**: No code execution, comprehensive DoS protection
- **Feature parity or better**: 100% compatibility plus intentional improvements

**Current Status**: See [SUPPORTED_FORMATS.md](SUPPORTED_FORMATS.md) for the full list of supported ecosystems and formats.

## Core Principles

### 1. Correctness Above All

> "always prefer correctness and full feature parity over effort/pragmatism"

- Every feature, edge case, and requirement from Python ScanCode must be preserved
- Zero tolerance for bugs - identify and fix issues from the original
- Comprehensive test coverage (unit + golden tests against Python reference)

### 2. Security First

- **No code execution**: AST parsing only, never eval/exec
- **DoS protection**: Explicit limits on file size, recursion, iterations
- **Archive safety**: Zip bomb prevention, compression ratio validation
- **Input validation**: Robust error handling, graceful degradation

See [ADR 0004: Security-First Parsing](adr/0004-security-first-parsing.md) for details.

### 3. Extraction vs Detection Separation

**Critical separation of concerns:**

- **Parsers extract** raw data from manifests
- **Detection engines** (future) normalize and analyze

Parsers NEVER:

- Normalize licenses to SPDX (detection engine's job)
- Extract copyright holders from file content (detection engine's job)
- Populate `declared_license_expression` (detection engine's job)

See [ADR 0002: Extraction vs Detection Separation](adr/0002-extraction-vs-detection.md) for details.

## System Architecture Overview

### Complete Processing Pipeline

scancode-rust implements a multi-phase processing pipeline based on Python ScanCode's architecture:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ScanCode Processing Pipeline                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Phase 1: Pre-Scan                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Archive extraction                                    â”‚    â”‚
â”‚  â”‚ â€¢ File type detection                                   â”‚    â”‚
â”‚  â”‚ â€¢ Pre-processing hooks                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚  Phase 2: Scanning                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Package manifest parsing (see SUPPORTED_FORMATS.md)   â”‚    â”‚
â”‚  â”‚ â€¢ License text detection                                â”‚    â”‚
â”‚  â”‚ â€¢ Copyright detection                                   â”‚    â”‚
â”‚  â”‚ â€¢ Email/URL extraction                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚  Phase 3: Post-Processing                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Package assembly (sibling, nested, file-ref, workspace)â”‚    â”‚
â”‚  â”‚ â€¢ Package consolidation/deduplication                   â”‚    â”‚
â”‚  â”‚ â€¢ License/copyright summarization                       â”‚    â”‚
â”‚  â”‚ â€¢ Tallies and facets                                    â”‚    â”‚
â”‚  â”‚ â€¢ Classification                                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚  Phase 4: Filtering                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ License policy filtering                              â”‚    â”‚
â”‚  â”‚ â€¢ Custom filter plugins                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚  Phase 5: Output                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ JSON output (ScanCode-compatible)                     â”‚    â”‚
â”‚  â”‚ â€¢ SPDX (RDF, JSON, YAML, tag-value)                     â”‚    â”‚
â”‚  â”‚ â€¢ CycloneDX (JSON, XML)                                 â”‚    â”‚
â”‚  â”‚ â€¢ CSV, YAML, HTML                                       â”‚    â”‚
â”‚  â”‚ â€¢ Custom templates                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Inventory

- **Package Parsers**: See [SUPPORTED_FORMATS.md](SUPPORTED_FORMATS.md) for complete list
- **Scanner Pipeline**: File discovery, parallel processing, progress tracking
- **Security Layer**: DoS protection, no code execution, archive safety
- **Package Assembly**: Sibling and nested merge strategies for combining related manifests
- **Text Detection**: License detection, copyright detection, email/URL extraction
- **Post-Processing**: Summarization, tallies, classification
- **Output**: JSON (ScanCode-compatible), SPDX, CycloneDX, CSV, YAML, HTML
- **Testing Infrastructure**: Unit tests, doctests, golden tests, integration tests
- **Infrastructure**: Plugin system, caching, enhanced progress tracking

### Implementation Status

For current implementation status, priorities, and effort estimates, see:

- **[implementation-plans/README.md](implementation-plans/README.md)** - Overview of all implementation plans
- **[implementation-plans/package-detection/](implementation-plans/package-detection/)** - Package parsing and assembly
- **[implementation-plans/text-detection/](implementation-plans/text-detection/)** - License, copyright, email/URL detection
- **[implementation-plans/post-processing/](implementation-plans/post-processing/)** - Summarization and tallies
- **[implementation-plans/output/](implementation-plans/output/)** - Output format support
- **[implementation-plans/infrastructure/](implementation-plans/infrastructure/)** - Plugin system, caching, progress tracking

Each plan includes detailed status, priorities (P0-P3), effort estimates, and implementation phases.

### Plugin Architecture

Python ScanCode uses a plugin-based architecture with 5 plugin types:

1. **PreScan Plugins**: Archive extraction, file type detection
2. **Scan Plugins**: Package detection, license detection, copyright detection
3. **PostScan Plugins**: Package assembly, summarization, classification
4. **OutputFilter Plugins**: License policy filtering, custom filters
5. **Output Plugins**: Format-specific output (SPDX, CycloneDX, etc.)

The Rust implementation will adopt a similar architecture using Rust traits and dynamic dispatch, with compile-time plugin registration for zero runtime overhead.

## Architecture Components

### Trait-Based Parser System

**Core Abstraction:**

```rust
pub trait PackageParser {
    const PACKAGE_TYPE: &'static str;
    
    fn is_match(path: &Path) -> bool;
    fn extract_packages(path: &Path) -> Vec<PackageData>;
}
```

**Benefits:**

- Type-safe dispatch at compile time
- Zero runtime overhead
- Clear contract for all parsers
- Easy to test in isolation

**Implementation:**

```rust
pub struct NpmParser;

impl PackageParser for NpmParser {
    const PACKAGE_TYPE: &'static str = "npm";
    
    fn is_match(path: &Path) -> bool {
        matches!(
            path.file_name().and_then(|n| n.to_str()),
            Some("package.json" | "package-lock.json")
        )
    }
    
    fn extract_packages(path: &Path) -> Vec<PackageData> {
        // Implementation
    }
}
```

See [ADR 0001: Trait-Based Parser Architecture](adr/0001-trait-based-parsers.md) for details.

### Parser Registration System

**How parsers are wired to the scanner:**

Parsers and recognizers are registered via the `register_package_handlers!` macro in `src/parsers/mod.rs`:

```rust
register_package_handlers! {
    parsers: [
        NpmParser,
        NpmLockParser,
        CargoParser,
        CargoLockParser,
        // ... more parsers ...
    ],
    recognizers: [
        JavaJarRecognizer,
        // ... file-type recognizers ...
    ],
}
```

**What this macro generates:**

1. **`try_parse_file(path: &Path) -> Option<Vec<PackageData>>`**
   - Called by scanner for every file
   - Tries each parser's `is_match()` in order
   - Returns first match's extracted data

2. **`parse_by_type_name(type_name: &str, path: &Path) -> Option<PackageData>`**
   - Used by test utilities for golden test generation
   - Allows direct parser invocation by name

3. **`list_parser_types() -> Vec<&'static str>`**
   - Returns all registered parser type names
   - Used by integration tests to verify registration

**Critical:** If a parser is implemented but not listed in this macro, it will **never be called** by the scanner, even if fully implemented and tested. The integration test `test_all_parsers_are_registered_and_exported` verifies this.

### Unified Data Model

All parsers output a single `PackageData` struct:

```rust
pub struct PackageData {
    // Identity
    pub package_type: Option<String>,
    pub namespace: Option<String>,
    pub name: Option<String>,
    pub version: Option<String>,
    pub purl: Option<String>,
    pub datasource_id: Option<DatasourceId>,
    
    // Metadata
    pub description: Option<String>,
    pub primary_language: Option<String>,
    pub release_date: Option<String>,
    pub homepage_url: Option<String>,
    pub parties: Vec<Party>,
    pub keywords: Vec<String>,
    
    // Dependencies
    pub dependencies: Vec<Dependency>,
    
    // Licenses (extraction only - detection is separate)
    pub extracted_license_statement: Option<String>,
    pub declared_license_expression: Option<String>,
    pub license_detections: Vec<LicenseDetection>,
    
    // Checksums & URLs
    pub sha1: Option<String>,
    pub md5: Option<String>,
    pub sha256: Option<String>,
    pub sha512: Option<String>,
    pub download_url: Option<String>,
    pub vcs_url: Option<String>,
    pub repository_homepage_url: Option<String>,
    pub repository_download_url: Option<String>,
    pub api_data_url: Option<String>,
    
    // Additional data
    pub extra_data: Option<HashMap<String, serde_json::Value>>,
    pub source_packages: Vec<String>,
    pub file_references: Vec<FileReference>,
    pub is_private: bool,
    pub is_virtual: bool,
    // ... and more (see src/models/file_info.rs for complete definition)
}
```

**Rationale:**

- Normalizes differences across all supported ecosystems
- SBOM-compliant output format
- Single source of truth for structure

### Scanner Pipeline

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     scancode-rust                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  1. File Discovery           2. Parser Selection          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Walk directory â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Match file    â”‚           â”‚
â”‚  â”‚ Apply filters  â”‚          â”‚ to parser     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                      â”‚                     â”‚
â”‚  3. Extraction                       v                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ PackageParser::extract_packages()           â”‚           â”‚
â”‚  â”‚ â”€ Read manifest                            â”‚           â”‚
â”‚  â”‚ â”€ Parse structure                          â”‚           â”‚
â”‚  â”‚ â”€ Extract metadata                         â”‚           â”‚
â”‚  â”‚ â”€ Return PackageData                       â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                   â”‚                                        â”‚
â”‚  4. Output        v                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ JSON serialization                  â”‚                  â”‚
â”‚  â”‚ â”€ ScanCode Toolkit compatible       â”‚                  â”‚
â”‚  â”‚ â”€ SBOM-ready structure              â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                            â”‚
â”‚  Future: Detection Engines (Post-Parser)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ License Detection â”‚  â”‚ Copyright        â”‚             â”‚
â”‚  â”‚ â”€ SPDX normalize  â”‚  â”‚ Detection        â”‚             â”‚
â”‚  â”‚ â”€ Confidence      â”‚  â”‚ â”€ Holder extract â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Parallel Processing

Uses `rayon` for multi-threaded file scanning:

```rust
// Actual implementation in src/scanner/process.rs
files.par_iter()
    .map(|(path, metadata)| {
        // Each file processed in parallel
        let file_entry = process_file(path, metadata, scan_strategy);
        progress_bar.inc(1);
        file_entry
    })
    .collect()
```

Inside `process_file()`, the scanner calls `try_parse_file(path)` (generated by `register_package_handlers!` macro):

```rust
// src/scanner/process.rs, line 148
if let Some(package_data) = try_parse_file(path) {
    file_info_builder.package_data(package_data);
    Ok(())
} else {
    // Not a package manifest, try license detection
    extract_license_information(...)
}
```

**Benefits:**

- Utilizes all CPU cores
- Maintains thread safety (Rust ownership guarantees)
- Progress tracking with atomic operations

### Package Assembly System

After scanning, the assembly system merges related manifests into logical packages using `DatasourceId`-based matching.

**Four assembly passes:**

- **SiblingMerge**: Combines sibling files in the same directory (e.g., `package.json` + `package-lock.json` â†’ single npm package)
- **NestedMerge**: Combines parent/child manifests across directories (e.g., Maven parent POM + module POMs)
- **FileRefResolve**: Resolves `file_references` from package database entries (RPM/Alpine/Debian) against scanned files, sets `for_packages` on matched files, tracks missing references, and resolves RPM namespace from os-release
- **WorkspaceMerge**: Post-processing pass for monorepo workspaces (e.g., npm/pnpm/Cargo workspaces â†’ separate Package per workspace member with shared resource assignment and `workspace:*` version resolution)

**How it works:**

1. Each `AssemblerConfig` declares which `DatasourceId` variants belong together and which file patterns to look for
2. After scanning, the assembler groups packages by directory
3. Packages whose `datasource_id` values match the same config are merged into a single logical package
4. Combined packages aggregate `datafile_paths` and `datasource_ids` from all contributing files
5. File reference resolution matches installed-package database entries to files on disk (e.g., Alpine `installed` DB lists files belonging to each package)
6. Workspace assembly runs as a final pass: detects workspace roots (npm/pnpm/Cargo), discovers members via glob patterns, creates per-member packages with full metadata inheritance (Cargo `[workspace.package]` and `workspace = true` resolution), hoists dependencies, and resolves `workspace:*` version references

Assembly is configurable via the `--no-assemble` CLI flag. See `src/assembly/` for implementation details.

### Security Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Security Layers                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Layer 1: No Code Execution                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ AST parsing only (setup.py, build.gradle)      â”‚    â”‚
â”‚  â”‚ Never eval/exec/subprocess                      â”‚    â”‚
â”‚  â”‚ Regex/token-based for DSLs                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  Layer 2: Resource Limits                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ File size: 100MB max                            â”‚    â”‚
â”‚  â”‚ Recursion depth: 50 levels                      â”‚    â”‚
â”‚  â”‚ Iterations: 100,000 max                         â”‚    â”‚
â”‚  â”‚ String length: 10MB per field                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  Layer 3: Archive Safety                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Uncompressed size: 1GB max                      â”‚    â”‚
â”‚  â”‚ Compression ratio: 100:1 max (zip bomb detect)  â”‚    â”‚
â”‚  â”‚ Path traversal: Block ../ patterns              â”‚    â”‚
â”‚  â”‚ Temp cleanup: Automatic via TempDir             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  Layer 4: Input Validation                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Result<T, E> error handling                     â”‚    â”‚
â”‚  â”‚ No .unwrap() in library code                    â”‚    â”‚
â”‚  â”‚ Graceful degradation on errors                  â”‚    â”‚
â”‚  â”‚ UTF-8 validation with lossy fallback            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

See [ADR 0004: Security-First Parsing](adr/0004-security-first-parsing.md) for comprehensive security analysis.

## Testing Strategy

### Four-Layer Test Pyramid

```text
         /\
        /  \    Integration Tests
       /    \   â”€ End-to-end scanner pipeline
      /------\  â”€ Full scan validation
     /        \
    / Golden   \ Golden Tests
   /  Tests     \ â”€ Compare with Python ScanCode output
  /--------------\ â”€ Real-world manifest files
 /                \
/    Unit Tests    \ Unit Tests + Doctests
/   + Doctests      \ â”€ Parser functions, edge cases
/____________________\ â”€ API documentation examples
```

**Four layers** (see [TESTING_STRATEGY.md](TESTING_STRATEGY.md) for full details):

1. **Doctests**: API documentation examples that run as tests
2. **Unit Tests**: Component-level tests for individual functions and edge cases
3. **Golden Tests**: Regression tests comparing output against Python ScanCode reference
4. **Integration Tests**: End-to-end tests validating the full scanner pipeline

See [ADR 0003: Golden Test Strategy](adr/0003-golden-test-strategy.md) for golden test details.

## Documentation Strategy

### Three-Layer Documentation

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Documentation Sources                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                    â”‚                  â”‚
           â–¼                    â–¼                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Parser    â”‚     â”‚ Doc Comments â”‚   â”‚   Manual   â”‚
    â”‚  Metadata   â”‚     â”‚   (/// //!)  â”‚   â”‚ Markdown   â”‚
    â”‚   (code)    â”‚     â”‚              â”‚   â”‚   Files    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚                   â”‚                   â”‚
           â–¼                   â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Auto-Gen    â”‚     â”‚  cargo doc   â”‚   â”‚   GitHub   â”‚
    â”‚ Formats.md  â”‚     â”‚  (docs.rs)   â”‚   â”‚   README   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Auto-Generated**: `docs/SUPPORTED_FORMATS.md` (from parser metadata)  
**API Reference**: cargo doc (from `///` and `//!` comments)  
**Architecture**: ADRs, improvements, guides (manual Markdown)

See [ADR 0005: Auto-Generated Documentation](adr/0005-auto-generated-docs.md) for details.

## Beyond-Parity Improvements

We don't just match Python ScanCode - we improve it:

| Parser | Improvement | Type |
|--------|-------------|------|
| **Alpine** | SHA1 checksums correctly decoded + Provider field extraction | ğŸ› Bug Fix + âœ¨ Feature |
| **RPM** | Full dependency extraction with version constraints | âœ¨ Feature |
| **Debian** | .deb archive introspection | âœ¨ Feature |
| **Conan** | conanfile.txt and conan.lock parsers (Python has neither) | âœ¨ Feature |
| **Gradle** | No code execution (token lexer vs Groovy engine) | ğŸ›¡ï¸ Security |
| **Gradle Lockfile** | gradle.lockfile parser (Python has no equivalent) | âœ¨ Feature |
| **npm Workspace** | pnpm-workspace.yaml extraction + workspace assembly with per-member packages (Python has stub parser + basic assembly) | âœ¨ Feature |
| **Cargo Workspace** | Full `[workspace.package]` metadata inheritance + `workspace = true` dependency resolution (Python has basic assembly) | âœ¨ Feature |
| **Composer** | Richer provenance metadata (7 extra fields) | ğŸ” Enhanced |
| **Ruby** | Semantic party model (unified name+email) | ğŸ” Enhanced |
| **Dart** | Proper scope handling + YAML preservation | ğŸ” Enhanced |
| **CPAN** | Full metadata extraction (Python has stubs only) | âœ¨ Feature |

See [docs/improvements/](improvements/) for detailed documentation of each improvement.

## Project Structure

The codebase follows a modular architecture:

- **`src/parsers/`** - Package manifest parsers (one per ecosystem)
- **`src/models/`** - Core data structures (PackageData, Dependency, DatasourceId, etc.)
- **`src/assembly/`** - Package assembly system (merging related manifests)
- **`src/scanner/`** - File system traversal and orchestration
- **`docs/`** - Architecture decisions, improvement docs, and guides
- **`testdata/`** - Test manifests for validation
- **`reference/`** - Python ScanCode Toolkit (reference submodule)

## Performance Characteristics

### Benchmarks

*(To be added: criterion benchmarks for parser performance)*

### Optimization Strategies

1. **Parallel Processing**: Uses all CPU cores via rayon
2. **Zero-Copy Parsing**: `&str` instead of `String` where possible
3. **Compile-Time Embedding**: License data embedded via `include_dir!`
4. **Lazy Evaluation**: Iterators instead of eager Vec building
5. **Efficient Parsers**: quick-xml, toml, serde_json (production-grade)

### Release Optimizations

```toml
[profile.release]
lto = true                # Link-time optimization
codegen-units = 1         # Single codegen unit for max optimization
strip = true              # Strip symbols for smaller binary
opt-level = 3             # Maximum optimization
```

## Extended Architecture

The following sections describe major architectural components in detail. See [implementation-plans/](implementation-plans/) for implementation status and roadmap.

### Text Detection Engines

**License Detection**:

- License text matching using fingerprinting algorithms
- SPDX license expression generation
- Confidence scoring and multi-license handling
- Integration with existing SPDX license data

**Copyright Detection** (see [COPYRIGHT_DETECTION_PLAN.md](implementation-plans/text-detection/COPYRIGHT_DETECTION_PLAN.md)):

The copyright detection engine extracts copyright statements, holder names, and author information from source files using a four-stage pipeline:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Text     â”‚â”€â”€â”€>â”‚  2. Candidateâ”‚â”€â”€â”€>â”‚  3. Lex +    â”‚â”€â”€â”€>â”‚  4. Tree     â”‚
â”‚  Preparation â”‚    â”‚  Selection   â”‚    â”‚  Parse       â”‚    â”‚  Walk +      â”‚
â”‚              â”‚    â”‚              â”‚    â”‚              â”‚    â”‚  Refinement  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Text Preparation**: Normalizes copyright symbols (`Â©`, `(c)`, HTML entities), strips comment markers and markup, converts to ASCII
2. **Candidate Selection**: Filters lines using hint markers (`opyr`, `auth`, `Â©`, year patterns), groups multi-line statements, filters gibberish
3. **Lexing + Parsing**: POS-tags tokens via ~500 regex patterns (type-safe `PosTag` enum), then applies ~200 grammar rules to build parse trees identifying `COPYRIGHT`, `AUTHOR`, `NAME`, `COMPANY` structures
4. **Tree Walk + Refinement**: Extracts `CopyrightDetection`, `HolderDetection`, `AuthorDetection` from parse trees, applies cleanup (strip unbalanced parens, deduplicate "Copyright" words, filter junk)

Key design decisions vs Python reference:

- **Type-safe POS tags**: Enum-based (not string-based) â€” compiler catches tag typos
- **Thread-safe**: No global mutable state (Python uses a singleton `DETECTOR`)
- **`RegexSet`-based lexer**: Parallel multi-pattern matching vs Python's sequential scan
- **Extended year range**: 1960-2099 (Python stops at 2039)
- **Bug fixes**: Fixed year-year separator bug, duplicate patterns, `is_private_ip` IPv6 bug

Special cases handled:

- Linux CREDITS files (structured `N:/E:/W:` format)
- SPDX-FileCopyrightText and SPDX-FileContributor
- "All Rights Reserved" in English, German, French, Spanish, Dutch
- Multi-line copyright statements spanning consecutive lines

Module location: `src/copyright/`

**Email/URL Detection** (see [EMAIL_URL_DETECTION_PLAN.md](implementation-plans/text-detection/EMAIL_URL_DETECTION_PLAN.md)):

The email/URL detection engine is the simplest text detection feature â€” regex-based extraction with an ordered filter pipeline to remove junk results.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Read     â”‚â”€â”€â”€>â”‚  2. Regex    â”‚â”€â”€â”€>â”‚  3. Filter   â”‚â”€â”€â”€>â”‚  4. Yield    â”‚
â”‚  Lines       â”‚    â”‚  Match       â”‚    â”‚  Pipeline    â”‚    â”‚  Results     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Email detection**: RFC-ish regex (`[A-Z0-9._%-]+@[A-Z0-9.-]+\.[A-Z]{2,63}`) â†’ 3-step filter pipeline (junk domain filter, uninteresting email filter, dedup).

**URL detection**: Three regex alternatives (scheme URLs, bare-domain URLs, git-style URLs) â†’ 10-step filter pipeline:

1. CRLF cleanup â†’ trailing junk stripping â†’ empty URL filter â†’ scheme addition â†’ user/password stripping â†’ invalid URL filter â†’ canonicalization (via `url` crate) â†’ junk host filter â†’ junk URL filter â†’ dedup

Both support configurable thresholds (`--max-email N`, `--max-url N`, default 50).

Key design decisions vs Python reference:

- **`url` crate** for URL parsing/canonicalization (replaces `urlpy`)
- **`std::net`** for IP classification (replaces `ipaddress`)
- **Extended TLD support**: `{2,63}` per RFC 1035 (Python's `{2,4}` rejects `.museum`, `.technology`)
- **Fixed IPv6 private detection**: Python has assignment bug making IPv6 private detection non-functional
- **Proper error handling**: No silent exception swallowing in URL canonicalization

Junk classification data (~150 entries): example domains, private IPs, W3C/XML namespaces, DTD URLs, PKI/certificate URLs, CDN URLs, image file suffixes.

Module location: `src/finder/`

### Post-Processing Pipeline

**Package Consolidation**:

- Package deduplication across scan results
- Dependency graph resolution
- Transitive dependency handling

**Summarization**:

- License tallies and facets
- Copyright holder aggregation
- File classification (source, docs, data, etc.)
- Summary statistics

### Output Format Support

**SBOM Formats**:

- SPDX: RDF, JSON, YAML, tag-value
- CycloneDX: JSON, XML
- Compatibility with SBOM tooling ecosystem

**Additional Formats**:

- CSV (tabular data export)
- YAML (human-readable)
- HTML (interactive reports)
- Custom templates (user-defined formats)

#### Infrastructure Enhancements

**Plugin System**:

- Extensible plugin architecture
- Custom scan plugins
- Custom output formats
- Third-party integrations

**Caching** (see [CACHING_PLAN.md](implementation-plans/infrastructure/CACHING_PLAN.md)):

Two-layer caching system for scan performance optimization:

1. **License Index Cache**: Persists the compiled askalono `Store` (MessagePack + zstd) to avoid rebuilding from SPDX text on each run. Existing `Store::from_cache()`/`to_cache()` infrastructure handles serialization. Version-stamped with tool version + SPDX data version. Expected speedup: 200-300ms â†’ 20-50ms startup.

2. **Scan Result Cache** (beyond-parity â€” Python has none): Content-addressed per-file cache keyed by SHA256 hash (already computed in `process_file()`). Cached data: package_data, license_detections, copyrights, programming_language. Path-dependent fields reconstructed at load time. Sharded directory layout (`ab/ab3f...postcard`) for filesystem scalability. Expected speedup: 10-50x on repeated scans.

3. **Incremental Scanning** (beyond-parity â€” Python has none): Scan manifest tracks `{path: (mtime, size, sha256)}` per directory. On re-scan, only files with changed mtime/size are re-hashed and re-scanned. Enables CI/CD integration (scan only changed files per commit).

Cache location: XDG-compliant (`~/.cache/scancode-rust/`), overridable via `SCANCODE_RUST_CACHE` env var or `--cache-dir` CLI flag. Multi-process safety via `fd-lock` file locking. Atomic writes (temp + rename) prevent corruption on crash.

Module location: `src/cache/`

**Progress Tracking** (see [PROGRESS_TRACKING_PLAN.md](implementation-plans/infrastructure/PROGRESS_TRACKING_PLAN.md)):

Centralized `ScanProgress` struct managing multi-phase progress bars via `indicatif::MultiProgress`:

1. **Discovery phase**: Spinner while counting files. Records initial file/dir/size counts.
2. **Scan phase**: Main progress bar with ETA, elapsed time, and file count. Integrates with rayon parallel processing via `Arc<ProgressBar>`. Rate-limited to 20 Hz (indicatif default).
3. **Assembly phase**: Progress bar for package assembly (sibling merge, workspace merge, etc.).
4. **Scan summary**: Files/sec, bytes/sec, error count, per-phase timings, initial/final counts.

Three verbosity modes: `--quiet` (hidden draw target, suppresses all stderr), default (progress bars + summary), `--verbose` (file-by-file listing + extended summary). Mutually exclusive via `clap` conflicts.

Logging integration via `indicatif-log-bridge`: parser `warn!()` messages route above the progress bar without corrupting display. All progress goes to stderr; stdout reserved for structured output.

Module location: `src/progress.rs`

### Quality Enhancements

Ongoing quality improvements:

- Property-based testing with proptest
- Fuzzing with cargo-fuzz
- Performance benchmarks with criterion
- Memory profiling and optimization
- Continuous golden test expansion

## License Data Architecture

### How License Detection Works

This tool uses the [SPDX License List Data](https://github.com/spdx/license-list-data) for license detection. The license data is:

1. **Stored in a Git submodule** at `resources/licenses/` (sparse checkout of `json/details/` only)
2. **Embedded at compile time** using Rust's `include_dir!` macro (see `src/main.rs`)
3. **Built into the binary** - no runtime dependencies on external files

This means:

- **For users**: The binary is self-contained and portable
- **For developers**: The submodule must be initialized before building
- **Package size**: Only the needed JSON files are included in the published crate

### Updating the License Data

**For Releases:** The `release.sh` script automatically updates the license data to the latest version before publishing. No manual action needed.

**For Development:**

To initialize or update to the latest SPDX license definitions:

```sh
./setup.sh                  # Initialize/update license data to latest
cargo build --release       # Rebuild with updated data
```

The script will show if the license data was updated. If so, commit the change:

```sh
git add resources/licenses
git commit -m "chore: update SPDX license data"
```

The `setup.sh` script:

- Initializes the submodule with shallow clone (`--depth=1`)
- Configures sparse checkout to only include `json/details/` (saves ~90% disk space)
- Updates to the latest upstream version
- The build process then embeds these files directly into the compiled binary

## Related Documentation

- [README.md](../README.md) - User-facing overview, installation, and usage
- [AGENTS.md](../AGENTS.md) - Contributor guidelines and code style
- [ADRs](adr/) - Architectural decision records
- [Improvements](improvements/) - Beyond-parity features
- [SUPPORTED_FORMATS.md](SUPPORTED_FORMATS.md) - Complete format list (auto-generated)
- [Implementation Plans](implementation-plans/) - Feature implementation status and roadmap
